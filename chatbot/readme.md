### Chatbot with RAG

#### Demo
- I have provided my resume and coding standards for python as reference documents for the chatbot and loaded
- Documents are read and converted to plain text. Text is split into smaller chunks. Chunks are converted to embeddings using sentence-transformers then Embeddings are stored in ChromaDB for semantic search
- Question is converted to embedding vector then ChromaDB compares this vector with all stored document chunk embeddings and Returns the 3 most semantically similar document chunks
- Now the response is generated by LLM by the use of context and query
- For previous conversation, The history added to contextualize query. The process repeat

![1768298973776](image/Readme/1768298973776.png)